<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A Cautionary Tale of Assumptions in ML | Samuel’s Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="A Cautionary Tale of Assumptions in ML" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Or, How Automatic Augmentation Research Forgot to Consider Baselines." />
<meta property="og:description" content="Or, How Automatic Augmentation Research Forgot to Consider Baselines." />
<link rel="canonical" href="https://uncoolis.cool/ml/ai/augmentations/meta-learning/2021/03/24/assumptions.html" />
<meta property="og:url" content="https://uncoolis.cool/ml/ai/augmentations/meta-learning/2021/03/24/assumptions.html" />
<meta property="og:site_name" content="Samuel’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-24T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"A Cautionary Tale of Assumptions in ML","dateModified":"2021-03-24T00:00:00-05:00","datePublished":"2021-03-24T00:00:00-05:00","description":"Or, How Automatic Augmentation Research Forgot to Consider Baselines.","mainEntityOfPage":{"@type":"WebPage","@id":"https://uncoolis.cool/ml/ai/augmentations/meta-learning/2021/03/24/assumptions.html"},"@type":"BlogPosting","url":"https://uncoolis.cool/ml/ai/augmentations/meta-learning/2021/03/24/assumptions.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://uncoolis.cool/feed.xml" title="Samuel's Blog" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A Cautionary Tale of Assumptions in ML | Samuel’s Blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="A Cautionary Tale of Assumptions in ML" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Or, How Automatic Augmentation Research Forgot to Consider Baselines." />
<meta property="og:description" content="Or, How Automatic Augmentation Research Forgot to Consider Baselines." />
<link rel="canonical" href="https://uncoolis.cool/ml/ai/augmentations/meta-learning/2021/03/24/assumptions.html" />
<meta property="og:url" content="https://uncoolis.cool/ml/ai/augmentations/meta-learning/2021/03/24/assumptions.html" />
<meta property="og:site_name" content="Samuel’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-24T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"A Cautionary Tale of Assumptions in ML","dateModified":"2021-03-24T00:00:00-05:00","datePublished":"2021-03-24T00:00:00-05:00","description":"Or, How Automatic Augmentation Research Forgot to Consider Baselines.","mainEntityOfPage":{"@type":"WebPage","@id":"https://uncoolis.cool/ml/ai/augmentations/meta-learning/2021/03/24/assumptions.html"},"@type":"BlogPosting","url":"https://uncoolis.cool/ml/ai/augmentations/meta-learning/2021/03/24/assumptions.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://uncoolis.cool/feed.xml" title="Samuel's Blog" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Samuel&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">A Cautionary Tale of Assumptions in ML</h1><p class="page-description">Or, How Automatic Augmentation Research Forgot to Consider Baselines.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-03-24T00:00:00-05:00" itemprop="datePublished">
        Mar 24, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      2 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#ML">ML</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#AI">AI</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#augmentations">augmentations</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#meta-learning">meta-learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p>In a recent project, I set out to improve training data in an automated fashion.
One way this is approached already is through learned augmentation policies, like AutoAugment or PBA. 
Therefore, I decided to focus on learned augmentation policies.</p>

<p>The original objective here is as follows. Given a specific model and a training setup for this model, what augmentation policy should I choose such that the validation accuracy at the end of a full training is high.
We cannot optimize for this objective directly in most circumstances since it is very expensive, every single evaluation of an augmentation policy requires training a full model.
Therefore, different people use different approximations to this objective. AutoAugment for example used smaller datasets, models, and fewer epochs, while some other methods used completely different objectives, e.g. Adversarial AutoAugment searches for policies that yield the worst possible performance on the training set or Fast AutoAugment searches for policies that yield high accuracies when applied to images at test-time.
The results for all these methods looked good, though.</p>

<p>So, I tried to add to this myself using my own again different approximations to the expensive original task. Just like the previous approaches: with some success.</p>

<p>But, isn’t it suspicious that so many different approximations of the original objective work well? Shouldn’t some work badly? For my approach, I could tweak a lot of hyper-parameters in many ways without a very bad impact.
At some point I went so far as to change my approach such that it just stands still and does not search at all (zeroed the learning rate): It still worked well!!!</p>

<p>This is the point at which you should get suspicious, I guess. From there on, I tried to remove as much as possible from the algorithm, like complicated combinations of augmentations and saw that it still worked well.</p>

<p>In the end, I arrived at something trivial. I call it TrivialAugment. I simply take an augmentation and apply it with a random strength. That is it.</p>

<p><img src="https://user-images.githubusercontent.com/9828297/112285487-a82f9c00-8c8a-11eb-83b7-cf3630e63c88.jpeg" width="500"></p>

<p>This works better than most previously proposed complicated methods like the above-mentioned methods, see our <a href="https://arxiv.org/abs/2103.10158">paper</a> for thorough experiments.</p>

<p>How could this be overseen in previous work? I don’t know. But I know that we as ML researchers should never forget to question our assumptions. Especially the big one: learning more components is always better. In most cases, it is better, and this is how machine learning evolved, but there certainly are approaches to learning things that do not perform well, as we have shown in this work.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="SamuelGabriel/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/ml/ai/augmentations/meta-learning/2021/03/24/assumptions.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>uncoolis.cool is such an uncool domain</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
