---
toc: true
layout: post
description: 
comments: true
categories: [markdown]
title: 
---

Some people say bigger human-generated datasets yield better models indefinetely. OpenAI even provided some [research](https://arxiv.org/abs/2001.08361) 
investigating this for language models. They found it to be true in the realm they tested in and with large enough models. Others say it matters
a lot which kinds of models we use and how we train them. Jeff Clune goes as far as [proclaiming](https://arxiv.org/abs/1905.10985) 
artificial training environments as a feasible way towards AGI. Gary Marcus deems it essential to integrate some kind of logic mechanism into AI models.

