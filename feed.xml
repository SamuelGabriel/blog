<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://samuelsblog.com/feed.xml" rel="self" type="application/atom+xml" /><link href="http://samuelsblog.com/" rel="alternate" type="text/html" /><updated>2021-05-04T03:25:02-05:00</updated><id>http://samuelsblog.com/feed.xml</id><title type="html">Samuel’s Blog</title><subtitle>Samuel's Blog</subtitle><entry><title type="html">A Cautionary Tale of Assumptions in ML</title><link href="http://samuelsblog.com/ml/ai/augmentations/meta-learning/2021/03/24/assumptions.html" rel="alternate" type="text/html" title="A Cautionary Tale of Assumptions in ML" /><published>2021-03-24T00:00:00-05:00</published><updated>2021-03-24T00:00:00-05:00</updated><id>http://samuelsblog.com/ml/ai/augmentations/meta-learning/2021/03/24/assumptions</id><content type="html" xml:base="http://samuelsblog.com/ml/ai/augmentations/meta-learning/2021/03/24/assumptions.html">&lt;p&gt;In a recent project, I set out to improve training data in an automated fashion.
One way this is approached already is through learned augmentation policies, like AutoAugment or PBA. 
Therefore, I decided to focus on learned augmentation policies.&lt;/p&gt;

&lt;p&gt;The original objective here is as follows. Given a specific model and a training setup for this model, what augmentation policy should I choose such that the validation accuracy at the end of a full training is high.
We cannot optimize for this objective directly in most circumstances since it is very expensive, every single evaluation of an augmentation policy requires training a full model.
Therefore, different people use different approximations to this objective. AutoAugment for example used smaller datasets, models, and fewer epochs, while some other methods used completely different objectives, e.g. Adversarial AutoAugment searches for policies that yield the worst possible performance on the training set or Fast AutoAugment searches for policies that yield high accuracies when applied to images at test-time.
The results for all these methods looked good, though.&lt;/p&gt;

&lt;p&gt;So, I tried to add to this myself using my own again different approximations to the expensive original task. Just like the previous approaches: with some success.&lt;/p&gt;

&lt;p&gt;But, isn’t it suspicious that so many different approximations of the original objective work well? Shouldn’t some work badly? For my approach, I could tweak a lot of hyper-parameters in many ways without a very bad impact.
At some point I went so far as to change my approach such that it just stands still and does not search at all (zeroed the learning rate): It still worked well!!!&lt;/p&gt;

&lt;p&gt;This is the point at which you should get suspicious, I guess. From there on, I tried to remove as much as possible from the algorithm, like complicated combinations of augmentations and saw that it still worked well.&lt;/p&gt;

&lt;p&gt;In the end, I arrived at something trivial. I call it TrivialAugment. I simply take an augmentation and apply it with a random strength. That is it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/9828297/112285487-a82f9c00-8c8a-11eb-83b7-cf3630e63c88.jpeg&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This works better than most previously proposed complicated methods like the above-mentioned methods, see our &lt;a href=&quot;https://arxiv.org/abs/2103.10158&quot;&gt;paper&lt;/a&gt; for thorough experiments.&lt;/p&gt;

&lt;p&gt;How could this be overseen in previous work? I don’t know. But I know that we as ML researchers should never forget to question our assumptions. Especially the big one: learning more components is always better. In most cases, it is better, and this is how machine learning evolved, but there certainly are approaches to learning things that do not perform well, as we have shown in this work.&lt;/p&gt;</content><author><name></name></author><summary type="html">In a recent project, I set out to improve training data in an automated fashion. One way this is approached already is through learned augmentation policies, like AutoAugment or PBA. Therefore, I decided to focus on learned augmentation policies.</summary></entry><entry><title type="html">Artificial Babies on the Internet</title><link href="http://samuelsblog.com/markdown/2020/07/19/nomoredatasets.html" rel="alternate" type="text/html" title="Artificial Babies on the Internet" /><published>2020-07-19T00:00:00-05:00</published><updated>2020-07-19T00:00:00-05:00</updated><id>http://samuelsblog.com/markdown/2020/07/19/nomoredatasets</id><content type="html" xml:base="http://samuelsblog.com/markdown/2020/07/19/nomoredatasets.html">&lt;p&gt;The most powerful machine learning models train on a lot of data. Where is this data from? It usually is from the internet and is then
filtered, labeled and curated by humans.  Assuming all data that current machine learning models are trained on is in some form available on the internet, you might need to ask for it on a specific forum, though. A major way forward towards more intelligent machines is to think about ways to allow ML models to
use the internet themselves to learn. This isn’t an access to the “real world”, but is much less expensive than having a robot incarnation of 
an ML model roll around in our 3-dimensional world. Is this 3-dimensional world as important as we think, anyways? Even we spend a pretty fat share of our time interacting with the internet, even though our interfaces to the internet are pretty bad. Compare how much data bandwidth your body has to interact with the world with the share of it that can be used to interact with the internet.&lt;/p&gt;

&lt;p&gt;A way to think about an ML model that learns from the internet directly is to look at it as a baby that is left to live on the internet. I would like to describe a three-step curriculum such a model might go through and relate it to human babies to give inspiration for how such a baby-on-the-internet model might learn. Each stage represents a new learning capability, but previously learned capabilities are not forgotten and still used when applicable.&lt;/p&gt;

&lt;p&gt;At first it will just experience whatever random things it comes across: articles, terms of service, spam, pictures of random noise, ingredient lists. Just like a small baby can’t choose what it will listen its parents discuss about or what it sees when it is carried around.&lt;/p&gt;

&lt;p&gt;In the second step preferences develop for what kind of data to consume. The model will look for data it can incorporate easily, but does not know yet, especially data that helps it understand the world better. It might look at Simple Wikipedia entries, simple images or listen to repetitive music. Similar to a baby that prefers to look into faces over looking into the sky.&lt;/p&gt;

&lt;p&gt;The last stage of progress is for the model to interact with the internet and its human users to learn by interaction.
In this last stage I imagine the model to participate in forum discussions, or tweet. The equivalent in humans is to be able to interact with the environment at first by noises or gestures for example, later using language.&lt;/p&gt;

&lt;p&gt;I see these stages not only as stages a model goes through during training, but as stages in research as well. Currently we only have stage one models, that learn from whatever they come across and we rarely train them on random samples from the internet directly, since the samples would be noisy and low quality compared to the handcrafted datasets. The next step will be to incorporate stage two learning capabilities, such that training on the internet directly becomes feasible for the first time: the model itself builds the high-quality dataset. 
Finally, at the last step we are able to build models that can interact with the internet, too.&lt;/p&gt;

&lt;p&gt;The purpose of this post is to draw a parallel between ML models in the online world and Sapiens models in the physical world to inspire more people to think about ML models as babies on the web. I really believe that progress can be made if we move forward from our current dataset-centric focus in the machine learning community.&lt;/p&gt;</content><author><name></name></author><summary type="html">The most powerful machine learning models train on a lot of data. Where is this data from? It usually is from the internet and is then filtered, labeled and curated by humans. Assuming all data that current machine learning models are trained on is in some form available on the internet, you might need to ask for it on a specific forum, though. A major way forward towards more intelligent machines is to think about ways to allow ML models to use the internet themselves to learn. This isn’t an access to the “real world”, but is much less expensive than having a robot incarnation of an ML model roll around in our 3-dimensional world. Is this 3-dimensional world as important as we think, anyways? Even we spend a pretty fat share of our time interacting with the internet, even though our interfaces to the internet are pretty bad. Compare how much data bandwidth your body has to interact with the world with the share of it that can be used to interact with the internet.</summary></entry></feed>